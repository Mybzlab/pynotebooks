{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "905936cd",
   "metadata": {},
   "source": [
    "# Interactive Pipeline for Tracking Robots\n",
    "## v2, using Hough-transform\n",
    "\n",
    "\n",
    "##### Luco Buise\n",
    "##### MSc Thesis: Radboud University\n",
    "\n",
    "22-05-2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18c6f17",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da5940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# change the following to %matplotlib notebook for interactive plotting\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series  # for convenience\n",
    "\n",
    "import cv2\n",
    "import trackpy as tp\n",
    "import ipywidgets as widgets\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "from ipywidgets import HBox, Textarea, interact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc06ae3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e476f6-b935-4a1f-8445-a00f04e3c38a",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b743bb6d-6b06-46fd-a965-43864789032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_circles(image, min_radius, max_radius, param1, param2, dp=1.2):\n",
    "    min_dist = int(0.9 * max_radius)\n",
    "\n",
    "    # apply Hough transform\n",
    "    circles = cv2.HoughCircles(image, cv2.HOUGH_GRADIENT, dp, min_dist,\n",
    "                               param1=param1, param2=param2,\n",
    "                               minRadius=min_radius, maxRadius=max_radius)\n",
    "    return circles\n",
    "    \n",
    "def capture_frame(video_obj, frame_num):\n",
    "    video_obj.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "    ret, frame = video_obj.read()\n",
    "    if not ret:\n",
    "        return None # if reached ending, stop\n",
    "    return frame\n",
    "\n",
    "def draw_circles(ax, circles):\n",
    "    if circles is not None:\n",
    "        for pt in circles[0]:\n",
    "            x, y, r = pt\n",
    "            circle = plt.Circle((x, y), r, color='r', fill=False)\n",
    "            ax.add_patch(circle)\n",
    "\n",
    "def crop_image(img,x0,y0,x1,y1):\n",
    "    return img[y0:y1,x0:x1,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071453cd",
   "metadata": {},
   "source": [
    "## Import video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3110db2d-957e-4050-8bb2-4bddcd2bc800",
   "metadata": {},
   "source": [
    "Change dir/video name below to select your video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27098afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose video\n",
    "video_dir = \"lab_recordings/cluster_dispersion_v3\"\n",
    "video_name = \"group_50_floor\"\n",
    "video_path = os.path.join(video_dir, video_name + \".mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fcba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "ret, frame = cap.read()\n",
    "if ret:\n",
    "    x_res = frame.shape[1]\n",
    "    y_res = frame.shape[0]\n",
    "else:\n",
    "    print(\"Video does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e2e303-3387-47b2-807b-eecc9805b2b0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Determine pixel to cm ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daea16f-c192-4a94-8718-b0b0755477d6",
   "metadata": {},
   "source": [
    "A new window will pop up when you run this cell. Select two points between which you know the distance (standard is 150cm, change below if different)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6954b702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate pixel to cm ratio using something you know the distance of\n",
    "object_length_cm = 150 # 30 cm for ruler\n",
    "\n",
    "# convert to RGB\n",
    "frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(\"In the just opened window, select two points you know the distance between.\")\n",
    "\n",
    "# helper function\n",
    "def click_event(event, x, y, flags, param):\n",
    "    global ix, iy, points\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        points.append((x, y))\n",
    "        if len(points) == 2:  # after two points are selected, calculate distance\n",
    "            cv2.line(frame, points[0], points[1], (0, 255, 0), 2)\n",
    "            cv2.putText(frame, 'Distance in pixels: {:.2f}'.format(np.linalg.norm(np.array(points[1]) - np.array(points[0]))), \n",
    "                        (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "            cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "points = []\n",
    "cv2.imshow(\"Frame\", frame)\n",
    "cv2.setMouseCallback(\"Frame\", click_event)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "px_distance = np.linalg.norm(np.array(points[1]) - np.array(points[0]))\n",
    "print(f\"Pixel distance between two points: {px_distance} pixels\")\n",
    "\n",
    "# calc px to cm ratio\n",
    "px_cm_ratio = object_length_cm / px_distance\n",
    "print(f\"Pixel to cm ratio: {px_cm_ratio} cm/pixel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b9fbff-6098-4b77-a0f8-c61950bc02f6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Determine cropping parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c444b2-85fc-465f-8bf9-0117d66f49e6",
   "metadata": {},
   "source": [
    "Use to interactive widget to determine how to crop the video, then save the found values in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279d24c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIDGET FOR CROPPING\n",
    "\n",
    "# function to crop and display the frame\n",
    "def update_crop(crop_x1, crop_x2, crop_y1, crop_y2):\n",
    "    # capture a frame from the video\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # reset to the first frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        # crop the frame\n",
    "        crop_frame = frame[crop_y1:crop_y2, crop_x1:crop_x2]\n",
    "        \n",
    "        # show the cropped frame\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(cv2.cvtColor(crop_frame, cv2.COLOR_BGR2RGB))  # convert from BGR to RGB\n",
    "        plt.show()\n",
    "\n",
    "# create interactive widgets\n",
    "crop_x1_slider = widgets.IntSlider(min=0, max=x_res, step=1, value=0, description='Crop X1')\n",
    "crop_x2_slider = widgets.IntSlider(min=0, max=x_res, step=1, value=x_res, description='Crop X2')\n",
    "crop_y1_slider = widgets.IntSlider(min=0, max=y_res, step=1, value=0, description='Crop Y1')\n",
    "crop_y2_slider = widgets.IntSlider(min=0, max=y_res, step=1, value=y_res, description='Crop Y2')\n",
    "\n",
    "# link the widgets with the update function\n",
    "interactive_plot = widgets.interactive(update_crop, \n",
    "                                       crop_x1=crop_x1_slider, \n",
    "                                       crop_x2=crop_x2_slider, \n",
    "                                       crop_y1=crop_y1_slider, \n",
    "                                       crop_y2=crop_y2_slider)\n",
    "\n",
    "# display the interactive widgets\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf4bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop the video if desired, using values found above\n",
    "crop_x = (511, 1301)\n",
    "crop_y = (131, 932)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da1efa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8c4b9a",
   "metadata": {},
   "source": [
    "## Interactive widget for annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57f66a5-a878-4db3-852f-a79cd1a7bf4d",
   "metadata": {},
   "source": [
    "Using the interactive widget, determine the annotation parameters. Once satisfied, copy them in the cell below the widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9691eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture(video_path)\n",
    "frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "canny_edge_thresh_default = 100\n",
    "circle_detection_thresh_default = 30\n",
    "radiusMin_default = 5\n",
    "radiusMax_default = 30\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "@interact(\n",
    "    frame_num=(0, frame_count - 1),\n",
    "    canny_edge_thresh=(1, 300, 1), # how sharp are the edges considered\n",
    "    circle_detection_thresh=(1, 100, 1), # lower = more sensitive, but can detect false positives.\n",
    "    radius_min=(1, 50, 1),\n",
    "    radius_max=(5, 100, 1),\n",
    ")\n",
    "def explore_video(frame_num=0, canny_edge_thresh=canny_edge_thresh_default,\n",
    "                  circle_detection_thresh=circle_detection_thresh_default, radius_min=radiusMin_default,\n",
    "                  radius_max=radiusMax_default):\n",
    "    \n",
    "    frame = capture_frame(video, frame_num)\n",
    "    cropped = crop_image(frame, crop_x[0], crop_y[0], crop_x[1], crop_y[1])\n",
    "    grey = cv2.cvtColor(cropped, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # black and white threshold\n",
    "    #_, bw = cv2.threshold(gray, black_white_thresh, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # detect circles using Hough transform\n",
    "    circles = detect_circles(grey, radius_min, radius_max, canny_edge_thresh, circle_detection_thresh)\n",
    "\n",
    "    # display result\n",
    "    with output:\n",
    "        output.clear_output(wait=True)\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        ax.imshow(grey, cmap='gray')\n",
    "        draw_circles(ax, circles)\n",
    "        ax.set_title(f\"Frame {frame_num}\")\n",
    "        plt.show()\n",
    "\n",
    "display(HBox([output]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b600da-515a-451f-a2bf-ac87c2999224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide on params using widget above\n",
    "canny_edge_thresh = 47\n",
    "circle_detection_thresh = 15\n",
    "radiusMin = 10\n",
    "radiusMax = 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb13ddc7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a89555",
   "metadata": {},
   "source": [
    "## Get and plot trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f786d0e9-dc54-4864-ac7c-44ed7872f48c",
   "metadata": {},
   "source": [
    "Run these cells to get all the trajectories and plot them. If you want to change when to start and end tracking, change the parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe94ccb9-d8f8-4ad3-b11f-c9d28d8e5362",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture(video_path)\n",
    "frameCount = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# change beginning and ending\n",
    "start_frame = 0\n",
    "end_frame = frameCount\n",
    "\n",
    "frames = np.array(range(start_frame, end_frame, 1))\n",
    "\n",
    "circle_detections = []\n",
    "\n",
    "for i, frame_num  in enumerate(frames):\n",
    "    img = capture_frame(video, frame_num )\n",
    "    if img is not None:\n",
    "        img = crop_image(img, crop_x[0], crop_y[0], crop_x[1], crop_y[1])\n",
    "        grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        circles = detect_circles(grayImage, radiusMin, radiusMax, canny_edge_thresh, circle_detection_thresh)\n",
    "    \n",
    "        if circles is not None:\n",
    "            for circle in circles[0]:  # circles[0] is the list of detections\n",
    "                x, y, r = circle\n",
    "                circle_detections.append([x, y, r, frame_num])\n",
    "    \n",
    "        if frame_num  % 500 == 0:\n",
    "            print(\"Done with frame\", frame_num , \"/\", frameCount) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ba41da-016d-497a-9d67-eaee9561c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#array to dataframe\n",
    "frame_df = pd.DataFrame(circle_detections, columns=[\"x\", \"y\", \"size\", \"frame\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8043ca-2f96-4537-82b4-a56ae3799ff8",
   "metadata": {},
   "source": [
    "### Change the parameters below to help determine how the trajectories are created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef30e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for trajectory creation\n",
    "max_dist = 10 # maximum distance particle can move between frames\n",
    "memory = 10 # maximum number of frames during which a feature can vanish, and be considered the same particle\n",
    "min_length = 100 # minimum length for a trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324b664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get trajectories from the locations\n",
    "t = tp.link(frame_df, max_dist, memory=memory)\n",
    "\n",
    "# remove short trajectories\n",
    "t1 = tp.filter_stubs(t, min_length)\n",
    "\n",
    "# compare the number of particles in the unfiltered and filtered data.\n",
    "print('Before stub filtering:', t['particle'].nunique())\n",
    "print('After stub filtering:', t1['particle'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb88abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "# frame size\n",
    "cropped_x_res = crop_x[1] - crop_x[0]\n",
    "cropped_y_res = crop_y[1] - crop_y[0]\n",
    "\n",
    "# make plot same shape&size as the frames\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([0, cropped_x_res])\n",
    "ax.set_ylim([0, cropped_y_res])\n",
    "\n",
    "# add scale bar\n",
    "scale_bar_length_cm = 30\n",
    "\n",
    "# convert scale bar length from cm to pixels (using ratio calculated in beginning)\n",
    "scale_bar_length_pxs = scale_bar_length_cm / px_cm_ratio\n",
    "\n",
    "# location of the scale bar\n",
    "x_scale_start = 50\n",
    "y_scale_start = cropped_y_res - 50\n",
    "scale_bar_end = x_scale_start + scale_bar_length_pxs\n",
    "\n",
    "# draw a horizontal line for the scale bar\n",
    "plt.plot([x_scale_start, scale_bar_end], [y_scale_start, y_scale_start], color='black', lw=2)\n",
    "\n",
    "# add label for the scale bar\n",
    "plt.text(x_scale_start + scale_bar_length_pxs / 2, y_scale_start - 5, f'{scale_bar_length_cm} cm', \n",
    "         color='black', ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "# plot traj\n",
    "tp.plot_traj(t1, label=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8c0286-29c8-4ed1-9ba1-2b0357a10505",
   "metadata": {},
   "source": [
    "### If you wish to remove any of the trajectories plotted above, add their particle numbers to the array below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd2fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any additional trajectories you do not want\n",
    "remove_part = []\n",
    "\n",
    "t2 = t1.copy()\n",
    "\n",
    "for p in remove_part:\n",
    "    t2 = t2[t2.particle != p]\n",
    "    \n",
    "plt.figure()\n",
    "\n",
    "# frame size\n",
    "cropped_x_res = crop_x[1] - crop_x[0]\n",
    "cropped_y_res = crop_y[1] - crop_y[0]\n",
    "\n",
    "# make plot same shape&size as the frames\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([0, cropped_x_res])\n",
    "ax.set_ylim([0, cropped_y_res])\n",
    "\n",
    "# add scale bar\n",
    "scale_bar_length_cm = 30\n",
    "\n",
    "# convert scale bar length from cm to pixels (using ratio calculated in beginning)\n",
    "scale_bar_length_pxs = scale_bar_length_cm / px_cm_ratio\n",
    "\n",
    "# location of the scale bar\n",
    "x_scale_start = 50\n",
    "y_scale_start = cropped_y_res - 50\n",
    "scale_bar_end = x_scale_start + scale_bar_length_pxs\n",
    "\n",
    "# draw a horizontal line for the scale bar\n",
    "plt.plot([x_scale_start, scale_bar_end], [y_scale_start, y_scale_start], color='black', lw=2)\n",
    "\n",
    "# add label for the scale bar\n",
    "plt.text(x_scale_start + scale_bar_length_pxs / 2, y_scale_start - 5, f'{scale_bar_length_cm} cm', \n",
    "         color='black', ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "# plot traj\n",
    "tp.plot_traj(t2, label=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bca3dd-a79c-48be-97c9-d796a1e21a02",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Smoothen trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb28360-f938-4eec-b4d5-bd63b040f0bd",
   "metadata": {},
   "source": [
    "Run these cells if you wish to smoothen the trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d2193-aa1e-4af2-accd-8e6c39c31872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_trajectories(df, window_length=10, polyorder=2):   \n",
    "    # make sure 'frame' is only a column, not an index level\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Make sure data is sorted by particle and frame\n",
    "    df = df.sort_values(['particle', 'frame']).reset_index(drop=True)\n",
    "    \n",
    "    # group by particle and smooth each trajectory\n",
    "    for pid, group in df.groupby('particle'):\n",
    "        n = len(group)\n",
    "        \n",
    "        # adjust window_length if too short\n",
    "        wl = min(window_length, n if n % 2 == 1 else n - 1)\n",
    "        if wl < polyorder + 2:\n",
    "            # If trajectory too short for savgol, just copy raw data\n",
    "            df.loc[group.index, 'x'] = group['x']\n",
    "            df.loc[group.index, 'y'] = group['y']\n",
    "            continue\n",
    "        \n",
    "        # smooth x and y separately\n",
    "        df.loc[group.index, 'x'] = savgol_filter(group['x'], wl, polyorder)\n",
    "        df.loc[group.index, 'y'] = savgol_filter(group['y'], wl, polyorder)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c73b98c-da38-4f4e-9dea-edf80d3b1e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothen\n",
    "t2 = smooth_trajectories(t2)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# frame size\n",
    "cropped_x_res = crop_x[1] - crop_x[0]\n",
    "cropped_y_res = crop_y[1] - crop_y[0]\n",
    "\n",
    "# make plot same shape&size as the frames\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([0, cropped_x_res])\n",
    "ax.set_ylim([0, cropped_y_res])\n",
    "\n",
    "# add scale bar\n",
    "scale_bar_length_cm = 30\n",
    "\n",
    "# convert scale bar length from cm to pixels (using ratio calculated in beginning)\n",
    "scale_bar_length_pxs = scale_bar_length_cm / px_cm_ratio\n",
    "\n",
    "# location of the scale bar\n",
    "x_scale_start = 50\n",
    "y_scale_start = cropped_y_res - 50\n",
    "scale_bar_end = x_scale_start + scale_bar_length_pxs\n",
    "\n",
    "# draw a horizontal line for the scale bar\n",
    "plt.plot([x_scale_start, scale_bar_end], [y_scale_start, y_scale_start], color='black', lw=2)\n",
    "\n",
    "# add label for the scale bar\n",
    "plt.text(x_scale_start + scale_bar_length_pxs / 2, y_scale_start - 5, f'{scale_bar_length_cm} cm', \n",
    "         color='black', ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "# plot traj\n",
    "tp.plot_traj(t2, label=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf833e8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc019d35",
   "metadata": {},
   "source": [
    "## Save trajectory dataframe and parameters as pickle using dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680a4d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dir\n",
    "save_dir = \"saved_trajectories_pkl\"\n",
    "file_name = video_name + \".pkl\"\n",
    "\n",
    "save_file_path = os.path.join(save_dir, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407e156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_dict = {\"traj_df\":t2, \"px_cm_ratio\":px_cm_ratio, \"x_res_px\":cropped_x_res, \"y_res_px\":cropped_y_res}\n",
    "\n",
    "# save dict as pickle\n",
    "try:\n",
    "    with open(save_file_path, 'wb') as file:\n",
    "        pickle.dump(traj_dict, file)\n",
    "except Exception as e:\n",
    "    print(f\"Error pickling dictionary: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584f6e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading a pickle file\n",
    "try:\n",
    "    with open(save_file_path, 'rb') as file:\n",
    "        read_pkl_dict = pickle.load(file)\n",
    "except Exception as e:\n",
    "    print(f\"Error unpickling dictionary: {e}\")\n",
    "\n",
    "read_pkl_dict[\"traj_df\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29132a7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da08cb13",
   "metadata": {},
   "source": [
    "## Save trajectory dataframe as CSV (does not include parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11b9dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dir\n",
    "save_dir = \"saved_trajectories_csv\"\n",
    "file_name = video_name + \".csv\"\n",
    "\n",
    "save_file_path = os.path.join(save_dir, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d9280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df as csv\n",
    "t2.to_csv(save_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c000dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading a csv file\n",
    "read_csv_df = pd.read_csv(save_file_path)\n",
    "\n",
    "read_csv_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e2353b-33c5-4252-ad90-f25ecc257ea5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791fe685-8ba3-4590-b606-252ad9f415ba",
   "metadata": {},
   "source": [
    "## Create video trajectory overlapping the tracking trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c40bfd-bb49-4ff6-af94-336d4cae5ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide where and how to save the file\n",
    "output_video_name = video_name + \"_trajs.mp4\"\n",
    "output_path = os.path.join('traj_videos', output_video_name)\n",
    "\n",
    "# parameters for video\n",
    "fading_trail = True\n",
    "max_trail = 100  # how many past positions to show if using fading trail\n",
    "\n",
    "line_colour = (0, 0, 255)\n",
    "line_thickness = 1\n",
    "\n",
    "# load data\n",
    "df = t2\n",
    "\n",
    "# video reading\n",
    "video = cv2.VideoCapture(video_path)\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "crop_width = crop_x[1] - crop_x[0]\n",
    "crop_height = crop_y[1] - crop_y[0]\n",
    "\n",
    "# output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (crop_width, crop_height))\n",
    "\n",
    "# fast-forward to start_frame\n",
    "video.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "frame_idx = 0\n",
    "trajectory_history = {}  # particle_id -> list of (x, y)\n",
    "\n",
    "for abs_frame_idx in range(start_frame, end_frame):\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # crop frame\n",
    "    cropped = frame[crop_y[0]:crop_y[1], crop_x[0]:crop_x[1]].copy()\n",
    "    \n",
    "    # get current frame's data\n",
    "    current_frame_data = df[df['frame'] == abs_frame_idx]\n",
    "\n",
    "    for _, row in current_frame_data.iterrows():\n",
    "        x, y = int(row['x']), int(row['y'])\n",
    "        r = int(row['size'])  # radius\n",
    "        p_id = int(row['particle'])\n",
    "\n",
    "        # add to trajectory history\n",
    "        if p_id not in trajectory_history:\n",
    "            trajectory_history[p_id] = []\n",
    "        trajectory_history[p_id].append((x, y))\n",
    "\n",
    "        # draw current robot as circle\n",
    "        #cv2.circle(cropped, (x, y), r, (0, 255, 0), -1)\n",
    "\n",
    "        if not fading_trail:\n",
    "            # draw full trajectory line\n",
    "            pts = trajectory_history[p_id]\n",
    "            for i in range(1, len(pts)):\n",
    "                cv2.line(cropped, pts[i - 1], pts[i], line_colour, line_thickness)\n",
    "        else:\n",
    "            overlay = cropped.copy()\n",
    "            pts = trajectory_history[p_id]\n",
    "    \n",
    "            for i in range(max(1, len(pts) - max_trail), len(pts)):\n",
    "                pt1 = pts[i - 1]\n",
    "                pt2 = pts[i]\n",
    "                alpha = (i - (len(pts) - max_trail)) / max_trail  # 0 to 1\n",
    "                color = (0, 0, int(255 * (1 - alpha)))  # fade from bright red to dark\n",
    "                cv2.line(overlay, pt1, pt2, color, line_thickness)\n",
    "    \n",
    "            # blend overlay with original cropped frame\n",
    "            cv2.addWeighted(overlay, 0.6, cropped, 0.4, 0, cropped)\n",
    "\n",
    "    out.write(cropped)\n",
    "    frame_idx += 1\n",
    "\n",
    "video.release()\n",
    "out.release()\n",
    "print(\"Video saved to:\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b2ef5a-f233-463d-b913-25c305c52f71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
